# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

MAPF-GPT is an imitation learning system for Multi-Agent Pathfinding (MAPF). A GPT-style transformer is trained on expert trajectories generated by the LaCAM solver (C++) to learn decentralized pathfinding policies. Models are evaluated on the POGEMA benchmark.

## Environment Setup

Use `uv` with Python 3.10:
```bash
uv venv --python 3.10
source .venv/bin/activate
uv pip install -r docker/requirements.txt
```

Docker alternative:
```bash
cd docker && sh build.sh
```

## Key Commands

```bash
# Run inference example (downloads weights automatically from Hugging Face)
python example.py --map_name validation-mazes-seed-000 --model 2M --num_agents 32

# Apple Silicon: add --device mps
python example.py --map_name wfi_warehouse --model 85M --num_agents 192 --device mps

# List available map names
python example.py --show_map_names

# Run full POGEMA benchmark evaluation
python benchmark.py

# Train (single GPU)
torchrun --standalone --nproc_per_node=1 train.py gpt/config-6M.py

# Train (multi-GPU, e.g. 4)
torchrun --standalone --nproc_per_node=4 train.py gpt/config-6M.py

# Download pre-generated 1B training dataset from Hugging Face
python download_dataset.py

# Generate dataset from scratch using LaCAM
python generate_dataset.py
```

## Architecture

### Data Flow

**Training pipeline:**
```
dataset_configs/ → generate_dataset.py
  → POGEMA environments + LaCAM solver (lacam/)
  → tokenizer/ (observation encoding)
  → PyArrow .arrow files (dataset/)
  → train.py (DDP training loop)
  → checkpoints (out/)
```

**Inference pipeline:**
```
Hugging Face weights → MAPFGPTInference (gpt/inference.py)
  → tokenizer/ + cost2go C++ module
  → GPT model (gpt/model.py)
  → actions → POGEMA environment (create_env.py)
  → SVG animation (svg/)
```

### Core Components

**`gpt/model.py`** — Transformer architecture:
- Non-causal self-attention (Flash Attention supported with PyTorch >= 2.0)
- Block size: 161 tokens, vocab: 67 tokens
- Embedding weights tied to language head (GPT-2 style)
- Model sizes: 2M (CPU-friendly), 6M, 85M

**`gpt/inference.py`** — `MAPFGPTInference` class:
- Downloads weights from `aandreychuk/MAPF-GPT` on Hugging Face Hub
- Maintains per-agent action history across timesteps
- Masks to 5 possible actions; uses cost-to-go heuristics from C++ module

**`tokenizer/tokenizer.py`** — Observation encoder:
- Encodes relative positions, goal directions, action history, cost-to-go grid per agent
- Vocab of 67 tokens; critical parameters in `tokenizer/parameters.py` (radius=5, history=5, num_agents=13)
- `cost2go.cpp` compiled as native extension for performance

**`lacam/`** — C++ expert solver compiled via CMake + pybind11:
- Exposes `run_lacam()` to Python
- Used only for dataset generation, not at inference time

**`gpt/fast_data_loader.py`** — `MapfArrowDataset`:
- Reads PyArrow format files; pre-allocates GPU memory
- Shards files across DDP workers automatically

**`create_env.py`** — POGEMA environment factory:
- `create_eval_env()` for benchmarking
- `create_logging_env()` for dataset generation (logs expert actions)

### Configuration System

Model/training configs are Python files in `gpt/` (`config-2M.py`, `config-6M.py`, `config-85M.py`). `gpt/configurator.py` merges config file values with CLI overrides (`--key=value` syntax).

Evaluation scenarios live in `eval_configs/` organized by type (random, mazes, warehouse, movingai, puzzles). Each subfolder has `maps.yaml` and a scenario YAML. Dataset generation configs are in `dataset_configs/`.

### Training Details

- DDP via `torchrun`; `RANK`/`LOCAL_RANK`/`WORLD_SIZE` set automatically
- Mixed precision: bfloat16 or float16
- Cosine LR decay with warmup; gradient accumulation configurable
- Checkpoints saved to `out/`; training resumes from latest checkpoint automatically
- Optional Weights & Biases logging

## Dependencies

Python: `torch>=2.0`, `pyarrow`, `huggingface_hub`, `pybind11==2.13.1`, `cppimport>=22.8.2`, `wandb`, `loguru`

Note: `pogema-toolbox` is vendored directly in `pogema_toolbox/` and `moving_ai_tiles/` (source from v0.1.0) for local modification. It is not installed via pip.

C++: CMake >= 3.16, Boost, pthreads (required to compile `lacam/` and `tokenizer/cost2go.cpp`)
